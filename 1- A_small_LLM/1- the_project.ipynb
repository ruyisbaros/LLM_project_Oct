{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import ipdb  # for debugging\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import platform, shutil  # detect platform type\n",
    "import requests, zipfile, io\n",
    "import math\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "import sentencepiece as spm  # For the tokenizer\n",
    "\n",
    "# These lines improve performance for Ampere Architecture (e.g: A100s)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "# Empty GPU cache memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A-) PREWORKING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-) Requirements installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wandb\n",
    "# 22af9a162cd0b2ad0d4643a01a00657222e874bd\n",
    "#!pip install jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the device being used\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-) Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Architecture Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers encoder inputs will be (8, 512, 384) # 8 batches, 512 features/token each iteration, 384 token vector\n",
    "batch_size = 8 # number of samples in each iteration\n",
    "context_size = 512  # how many words/tokens will be taken each iteration\n",
    "embedded_size = 384  # vector size for the each token\n",
    "n_layers = 7 # number of layers in the encoder layer\n",
    "n_heads = 7 # number of heads in the multi-head attention mechanism\n",
    "BIAS = True # if True, add bias to the output of the linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HyperParameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "dropout = 0.05 # randomly zero out some input units with probability dropout. This avoids overfitting.\n",
    "weight_decay = 0.01 # regularization parameter for weight decay. Smaller values will result in stronger regularization. like L1 or L2 regularization.\n",
    "grad_clip = 1.0 # avoid exploding gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iters = 10000\n",
    "eval_interval = 50 # each 50 iterations, the model will be evaluated on the validation set\n",
    "eval_iters = 3\n",
    "compile = False # if True, the model will be compiled before training\n",
    "checkpoint_dir = \"models/\"  # Where do we store checkpoints?\n",
    "checkpoint_fn = \"latest.pt\"\n",
    "# Name of checkpoint file to be saved during training\n",
    "checkpoint_load_fn = \"latest.pt\"\n",
    "load_pretrained = False  # Do we want to load a pretrained model to continue training?\n",
    "dtype = torch.bfloat16 # data type for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "inference = False # if True, the model will be used for inference\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-) wandb Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import wandb\n",
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\ML_Bootcamp\\LLM_spain\\1- A_small_LLM\\wandb\\run-20241012_231634-529ze5uq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ahmet-erdonmez77-dci/llm1/runs/529ze5uq' target=\"_blank\">llm120241012-231634</a></strong> to <a href='https://wandb.ai/ahmet-erdonmez77-dci/llm1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ahmet-erdonmez77-dci/llm1' target=\"_blank\">https://wandb.ai/ahmet-erdonmez77-dci/llm1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ahmet-erdonmez77-dci/llm1/runs/529ze5uq' target=\"_blank\">https://wandb.ai/ahmet-erdonmez77-dci/llm1/runs/529ze5uq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_log = True \n",
    "wandb_project = \"llm1\"\n",
    "wandb_run_name = \"llm1\" +datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)\n",
    "    \"\"\" wandb.config.update({\n",
    "        \"lr\": lr,\n",
    "        \"dropout\": dropout,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"grad_clip\": grad_clip,\n",
    "        \"train_iterations\": train_iterations,\n",
    "        \"eval_interval\": eval_interval,\n",
    "        \"eval_iters\": eval_iters,\n",
    "        \"compile\": compile,\n",
    "        \"checkpoint_path\": checkpoint_path,\n",
    "        \"checkpoint_fn\": checkpoint_fn,\n",
    "        \"checkpoint_load_fn\": checkpoint_load_fn,\n",
    "        \"dtype\": dtype,\n",
    "        \"inference\": inference,\n",
    "        \"device\": device\n",
    "    }) \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-) Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' files_url = \"https://ideami.com/llm_train\"\\n\\n# Downloading proceeds if we detect that one of the key files to download is not present\\nif not os.path.exists(f\"encoded_data.pt\"):\\n    print(\"Downloading files using Python\")\\n    response = requests.get(files_url)\\n    zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")\\nelse:\\n    print(\\n        \"you seem to have already downloaded the files. If you wish to re-download them, delete the encoded_data.pt file\"\\n    )\\n '"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" files_url = \"https://ideami.com/llm_train\"\n",
    "\n",
    "# Downloading proceeds if we detect that one of the key files to download is not present\n",
    "if not os.path.exists(f\"encoded_data.pt\"):\n",
    "    print(\"Downloading files using Python\")\n",
    "    response = requests.get(files_url)\n",
    "    zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")\n",
    "else:\n",
    "    print(\n",
    "        \"you seem to have already downloaded the files. If you wish to re-download them, delete the encoded_data.pt file\"\n",
    "    )\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 178255102 characters\n",
      "terms.\n",
      "For example, there are objects in two groups (as shown on the right). The objects are various shapes, where one group has 3 of them while the other has 2. When the two groups combine into one, the overall amount (sum) of the shapes become 5.\n",
      "\n",
      "Vertical Addition\n",
      "\n",
      "The animation above demonstrate\n"
     ]
    }
   ],
   "source": [
    "with open('data/wiki.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(f\"Dataset size: {len(text)} characters\")\n",
    "print(text[30000:30300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-) Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file=\"data/wiki_tokenizer.model\")  # spm.SentencePieceProcessor is more advanced than NLTK's word_tokenize\n",
    "vocab_size = sp.GetPieceSize()\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    return sp.Encode(s)\n",
    "def decode(s):\n",
    "    return sp.Decode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[310, 4031, 116, 2895, 1090, 570, 285, 1172, 599, 1853, 4039, 751, 264, 314, 817, 4049, 3429, 4051]\n",
      "The quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"The quick brown fox jumps over the lazy dog.\"))\n",
    "print(decode(encode(\"The quick brown fox jumps over the lazy dog.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tutor created \"encoded_data.pt\" for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/encoded_data.pt'):\n",
    "    data = torch.load('data/encoded_data.pt')\n",
    "else: # shows how to create encoded_data.pt\n",
    "    encoded_data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    torch.save(encoded_data, 'data/encoded_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4031,   13, 4061,  ...,   13,   13,   13]), 59211077)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-) Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data size: 59.21 Millions | Train data size: 53.29 Millions | Validation data size: 5.92\n"
     ]
    }
   ],
   "source": [
    "data_size = len(data)\n",
    "spl = int(0.9 * data_size)\n",
    "train_data = data[:spl] # 90% of the data for training\n",
    "val_data = data[spl:] # 10% of the data for validation\n",
    "\n",
    "print(f\"Total data size: {data_size/1e6:.2f} Millions | Train data size: {len(train_data)/1e6:.2f} Millions | Validation data size: {len(val_data)/1e6:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will get 8 times of 512 tokens in each batch (8,512)\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    inds = torch.randint(len(data) - context_size, (batch_size,)) # batch_size 8 and context_size 512. Each batch will have 8 examples. and each example will have 512 tokens.\n",
    "    X = torch.stack([data[i:i+context_size] for i in inds])  # If we did not do (len(data) - context_size) instead of len(data), we would get out of range error\n",
    "    y = torch.stack([data[i+1: i+context_size+1] for i in inds])# if above is 1000:1512 => this 1001:1513. we move window one token forward\n",
    "    \n",
    "    return X.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 512]), torch.Size([8, 512]))"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = get_batch(\"train\")\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- THE MAGIC IS WE FORWARD WINDOW 1 TOKEN AHEAD EACH TIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 682, 1429,  983,  302,  501, 1161, 1064,  293,  615,  261],\n",
      "       device='cuda:0')\n",
      "tensor([1429,  983,  302,  501, 1161, 1064,  293,  615,  261, 2108],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(x[0][:10])\n",
    "print(y[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B-) TRANSFORMER MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-) Define the transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedded size is 384, context_size is 512, vocab_size is 4096, n_heads is 7, BIAS is True, n_layers is 7\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedded_size) # 4096 x 384 each vocab will have 384 size vector\n",
    "        self.positions = nn.Embedding(context_size, embedded_size) # 512 x 384 we define positions of each token\n",
    "        self.blocks = nn.Sequential(*[Block(n_heads) for _ in range(n_layers)])# creates n_layers (7) times transformer blocks and each block has n_heads (7) heads\n",
    "        self.ln = nn.LayerNorm(embedded_size) # Z-transform\n",
    "        self.final_linear = nn.Linear(embedded_size,vocab_size, bias=BIAS)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module,nn.Linear):\n",
    "            #module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            torch.nn.init.normal_(module.weight, std=0.02, mean=0.0)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, std=0.02, mean=0.0)\n",
    "            \n",
    "    def forward(self, input, targets = None): \n",
    "        # BS = Batch Size, SL=sequence Length or context Size\n",
    "        loss = None\n",
    "        BS, SL = input.shape # for us 8 x 512\n",
    "        emb = self.embeddings(input) # for us 8 x 512 x 384\n",
    "        position_ = self.positions(torch.arange(SL, device=device)) # for us 512 x 384\n",
    "        \n",
    "        x = emb + position_  # for us 8 x 512 x 384\n",
    "        \n",
    "        x = self.blocks(x) # pass through transformer blocks. BS x SL x 384\n",
    "        x = self.ln(x)  # normalization. BS x SL x 384 (embedding size)\n",
    "\n",
    "        logits = self.final_linear(x)  # to device (cuda or cpu)  # pass through linear layer BS x SL x 4096 (vocab size)\n",
    "        #probs = torch.softmax(logits, dim=-1)  # get probabilities for each token. BS x SL x 4096\n",
    "        \n",
    "        \n",
    "        \n",
    "        if targets is not None:\n",
    "            BS, SL, VS = logits.shape\n",
    "            logits = logits.view(BS*SL, VS)  # BS*SL x 4096\n",
    "            targets = targets.view(BS*SL)  # BS*SL x 1\n",
    "            loss = F.cross_entropy(logits, targets)  # calculate loss for each token. BS*SL x 1\n",
    "            \n",
    "            # manually calculate loss\n",
    "            #counts = logits.exp()\n",
    "            #prob = counts / counts.sum(dim=-1, keepdim=True)\n",
    "            #loss2 = - prob[torch.arange(BS*SL), targets].log().mean()\n",
    "            \n",
    "            #if(not torch.allclose(loss, loss2)):\n",
    "            #print(f\"[Loss Difference] Pytorch: {loss.item()} vs Manual: {loss2.item()}\")\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    # Predicts next token based on previous tokens\n",
    "    def generate(self, input, max_length=500):\n",
    "        for _ in range(max_length):\n",
    "            input = input[:, -context_size:] # take last context_size, 512 of tokens \n",
    "            logits, _ = self(input) # (1, input_size, vocab_size)\n",
    "            logits = logits[:, -1, :]  # pick the last token's logits (1, vocab_size)\n",
    "            probs = F.softmax(logits, dim=-1) # get probabilities for each token (1, vocab_size)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)#.squeeze(1)  # sample the next token (1, 1)\n",
    "            input = torch.cat((input, next_token), dim=1)  # add the sampled token to the input\n",
    "            \n",
    "        return input\n",
    "        \n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = embedded_size // n_heads # 384 // 7 ~ 54\n",
    "        self.ma = MultiHeadAttention(n_heads, head_size) # 7 heads * 54\n",
    "        self.feed_forward = ForwardLayer(embedded_size)\n",
    "        self.ln1 = nn.LayerNorm(embedded_size) # Z-transform\n",
    "        self.ln2 = nn.LayerNorm(embedded_size) # Z-transform\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.ma(self.ln1(x))\n",
    "        x = x + self.feed_forward(self.ln2(x))\n",
    "        return x  \n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)]) # iterate for each head\n",
    "        self.combine = nn.Linear(n_heads*head_size, embedded_size, bias=BIAS) # (7*54 ,384)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1) # dim=-1 do this on last dimension. dim = 0 do this on first dimension.\n",
    "        # each head outputs (BS, SL, head_size)\n",
    "        x = self.combine(x)\n",
    "        x = self.dropout(x)\n",
    "        return x \n",
    "        \n",
    "        \n",
    "\n",
    "class ForwardLayer(nn.Module):\n",
    "    def __init__(self, embedded_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embedded_size, 6*embedded_size, bias=BIAS),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(6*embedded_size, embedded_size, bias=BIAS),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    \n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embedded_size, head_size, bias=BIAS)\n",
    "        self.k = nn.Linear(embedded_size, head_size, bias=BIAS)\n",
    "        self.v = nn.Linear(embedded_size, head_size, bias=BIAS)\n",
    "        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size))) # an upper triangular matrix of size (context_size, context_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        BS, SL, VS = x.shape\n",
    "        q = self.q(x) # BS, SL, 54 => # (8, 512, 54),\n",
    "        k = self.k(x) # BS, SL, 54 => # (8, 512, 54),\n",
    "        v = self.v(x) # BS, SL, 54 => # (8, 512, 54),\n",
    "        \n",
    "        attn_weights = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5  # k.transpose(-2,-1) = (8, 54, 512) so ==> (8,512,54) @ (8,54,512) = (8,512,512)\n",
    "        attn_weights = attn_weights.masked_fill(self.tril[:SL, :SL] == 0, float('-inf'))\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)  # (8,512,512)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        x = attn_weights @ v  # (8,512,512) @ (8,512,54) = (8,512,54)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TO SEE MANUALLY GENERATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' head_size = embedded_size // n_heads # 384 // 7 ~ 54\\nx,y = get_batch(\"train\")\\nprint(\"x and y shape: \",x.shape, y.shape) # inputs\\n\\nx = x.to(device)\\ny = y.to(device)\\nembeddings = nn.Embedding(vocab_size, embedded_size).to(device)\\npositions = nn.Embedding(context_size, embedded_size).to(device)\\nqueries = nn.Linear(embedded_size, head_size, bias=BIAS).to(device)\\nkeys = nn.Linear(embedded_size, head_size, bias=BIAS).to(device)\\nvalues = nn.Linear(embedded_size, head_size, bias=BIAS).to(device)\\ntril = torch.tril(torch.ones(context_size, context_size)).to(device) # an upper triangular matrix of size (context_size, context_size)\\n\\nemb = embeddings(x)\\npos = positions(torch.arange(context_size, device=device))\\n\\nx = emb + pos\\n\\n# Multi-Head Attention\\n\\nq = queries(x)\\nk = keys(x)\\nv = values(x)\\nprint(\"q, k and v shape: \",q.shape, k.shape, v.shape)  # (8, 512, 54),\\ntorch.set_printoptions(precision=2, sci_mode=False)\\nprint(q[0][0][:5])\\n\\nattn_weights = q @ k.transpose(-2,-1) * k.shape[-1]**0.5  # k.transpose(-2,-1) = (8, 54, 512) so ==> (8,512,54) @ (8,54,512) = (8,512,512)\\nattn_weights = attn_weights.masked_fill(tril[:context_size, :context_size] == 0, -float(\\'inf\\'))\\nattn_weights = F.softmax(attn_weights, dim=-1)  # (8,512,512)\\n\\nx = attn_weights @ v # (8,512,512) @ (8,512,54) = (8,512,54)\\nprint(x[0][0])\\nprint(attn_weights.shape)\\nprint(x.shape) '"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" head_size = embedded_size // n_heads # 384 // 7 ~ 54\n",
    "x,y = get_batch(\"train\")\n",
    "print(\"x and y shape: \",x.shape, y.shape) # inputs\n",
    "\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "embeddings = nn.Embedding(vocab_size, embedded_size).to(device)\n",
    "positions = nn.Embedding(context_size, embedded_size).to(device)\n",
    "queries = nn.Linear(embedded_size, head_size, bias=BIAS).to(device)\n",
    "keys = nn.Linear(embedded_size, head_size, bias=BIAS).to(device)\n",
    "values = nn.Linear(embedded_size, head_size, bias=BIAS).to(device)\n",
    "tril = torch.tril(torch.ones(context_size, context_size)).to(device) # an upper triangular matrix of size (context_size, context_size)\n",
    "\n",
    "emb = embeddings(x)\n",
    "pos = positions(torch.arange(context_size, device=device))\n",
    "\n",
    "x = emb + pos\n",
    "\n",
    "# Multi-Head Attention\n",
    "\n",
    "q = queries(x)\n",
    "k = keys(x)\n",
    "v = values(x)\n",
    "print(\"q, k and v shape: \",q.shape, k.shape, v.shape)  # (8, 512, 54),\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "print(q[0][0][:5])\n",
    "\n",
    "attn_weights = q @ k.transpose(-2,-1) * k.shape[-1]**0.5  # k.transpose(-2,-1) = (8, 54, 512) so ==> (8,512,54) @ (8,54,512) = (8,512,512)\n",
    "attn_weights = attn_weights.masked_fill(tril[:context_size, :context_size] == 0, -float('inf'))\n",
    "attn_weights = F.softmax(attn_weights, dim=-1)  # (8,512,512)\n",
    "\n",
    "x = attn_weights @ v # (8,512,512) @ (8,512,54) = (8,512,54)\n",
    "print(x[0][0])\n",
    "print(attn_weights.shape)\n",
    "print(x.shape) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [5, 6, 7]])"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).reshape(2, 4)\n",
    "arr[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "384//7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### go on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-) Generate a sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x and y shape:  torch.Size([8, 512]) torch.Size([8, 512])\n",
      "8.375\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(\"train\")\n",
    "print(\"x and y shape: \", x.shape, y.shape)  # inputs\n",
    "\n",
    "model = GPT()\n",
    "model = model.to(device)\n",
    "model = model.to(dtype)\n",
    "\n",
    "logits, loss = model(x, y)\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_sample(input):\n",
    "    t1 = torch.tensor(encode(input), dtype=torch.long, device=device) # (1, size of ids)\n",
    "    t1 = t1[None, :] # (1, size of ids)\n",
    "    newgen = model.generate(t1, max_length=64)[0].tolist()\n",
    "    \n",
    "    result = decode(newgen)\n",
    "    \n",
    "    print(result)\n",
    "    \n",
    "#generate_sample(\"The quick brown fox jumps over the lazy dog.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n",
       " torch.Size([10]),\n",
       " torch.Size([1, 10]))"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([2,3,4,5,6,7,8,9,10,11])\n",
    "a, a.shape, a.unsqueeze(0).shape,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-) TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x and y shape:  torch.Size([8, 512]) torch.Size([8, 512])\n",
      "(19.837954, 'Million parameters')\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(\"train\")\n",
    "print(\"x and y shape: \", x.shape, y.shape)  # inputs\n",
    "\n",
    "model = GPT()\n",
    "model = model.to(device)\n",
    "model = model.to(dtype)\n",
    "\n",
    "if compile:\n",
    "    print(\"Torch compiled successfully\")\n",
    "    model = torch.compile(model)\n",
    "\n",
    "print((sum(p.numel() for p in model.parameters()) / 1e6 , \"Million parameters\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate Loss Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"eval\"]:\n",
    "        l = torch.zeros(eval_iters)\n",
    "        \n",
    "        for i in range(eval_iters):\n",
    "            x,y = get_batch(split)\n",
    "            _, loss = model(x, y)\n",
    "            l[i] = loss\n",
    "            \n",
    "        out[split] = l.mean().item()\n",
    "        \n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 8.4375, 'eval': 8.4375}"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = calculate_loss()\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HyperParameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# Main Training Process\n",
    "#################################################################################\n",
    "\n",
    "# Set Weight Decay differently for different kinds of parameters\n",
    "# parameter dictionary where keys are parameter names, and values are the parameter themselves\n",
    "p_dict = {\n",
    "    p_name: p for p_name, p in model.named_parameters() if p.requires_grad\n",
    "}  # len: 370\n",
    "\n",
    "# isolate weight matrices as they benefit specially from weight decay\n",
    "weight_decay_p = [p for n, p in p_dict.items() if p.dim() >= 2]  # len: 171\n",
    "\n",
    "# isolate other parameters like bias parameters, that don't benefit from weight decay\n",
    "no_weight_decay_p = [p for n, p in p_dict.items() if p.dim() < 2]  # len: 199\n",
    "\n",
    "# store the parameter types in a list of dictionaries\n",
    "optimizer_groups = [\n",
    "    {\"params\": weight_decay_p, \"weight_decay\": weight_decay},\n",
    "    {\"params\": no_weight_decay_p, \"weight_decay\": 0.0},\n",
    "]\n",
    "\n",
    "# Declare optimizer, it helps us compute gradients, update parameters, manage learning rate, apply weight decay\n",
    "optimizer = torch.optim.AdamW(optimizer_groups, lr=lr, betas=(0.9, 0.99))\n",
    "# betas: control the exponential moving averages of the gradient and its square,\n",
    "# which are essential components of the Adam and AdamW optimization algorithms.\n",
    "\n",
    "# Declare scheduler to change learning rate through the training\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, train_iters, eta_min=lr / 10\n",
    ")\n",
    "# learning rate will descend till a minimum of a tenth of the lr\n",
    "\n",
    "start_iteration = 0\n",
    "best_val_loss = float(\"inf\")  # Track best loss value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(device.index(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path):\n",
    "    print(\"LLM - Loading model\")\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])  # Load parameters\n",
    "    optimizer.load_state_dict(\n",
    "        checkpoint[\"optimizer_state_dict\"]\n",
    "    )  # Load optimizer state\n",
    "    iteration = checkpoint[\"iteration\"]  # In what iteration did we save the model?\n",
    "    loss = checkpoint[\"loss\"]  # What was the last loss value?\n",
    "    print(f\"Loaded iter {iteration} with loss {loss}\")\n",
    "    return iteration, loss\n",
    "\n",
    "\n",
    "################# OPTIONAL : LOAD A PREVIOUS CHECKPOINT\n",
    "if os.path.exists(f\"{checkpoint_dir}/{checkpoint_load_fn}\") and load_pretrained:\n",
    "    start_iteration, loss = load_checkpoint(checkpoint_dir + checkpoint_load_fn)\n",
    "    best_val_loss = loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1==1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inference Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "if inference:\n",
    "    model.eval()\n",
    "    while True:\n",
    "        qs = input(\"Enter a text (q to quit): \")\n",
    "        \n",
    "        if qs == \"\":\n",
    "            continue\n",
    "        if qs == \"q\":\n",
    "            break\n",
    "        \n",
    "        generate_sample(qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "###################### TRAINING #################################\n",
    "#################################################################\n",
    "\n",
    "try:\n",
    "    for i in tqdm(range(start_iteration, train_iters)):\n",
    "        xb, yb = get_batch(\"train\")  # Get a new batch of data\n",
    "        logits, loss = model(xb, yb)  # Run the LLM and get the logits and the loss\n",
    "\n",
    "        if i % eval_interval == 0 or i == train_iters - 1:  # Calculate the loss\n",
    "            l = calculate_loss()\n",
    "            print(f\"\\n{i}: train loss: {l['train']} / val loss: {l['eval']}\")\n",
    "\n",
    "            # We do a quick test so that we observe the evolution through the training\n",
    "            # Remember that we use a very small dataset which doesn't include all topics\n",
    "            generate_sample(\"The mountain in my city is\")  # Generate a sample\n",
    "\n",
    "            if (\n",
    "                l[\"eval\"] < best_val_loss\n",
    "            ):  # If we improved the best loss, save a checkpoint\n",
    "                best_val_loss = l[\"eval\"]\n",
    "                print(\"[CHECKPOINT]: Saving with loss: \", best_val_loss)\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"model_state_dict\": model.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        \"loss\": best_val_loss,\n",
    "                        \"iteration\": i,\n",
    "                    },\n",
    "                    checkpoint_dir + checkpoint_fn,\n",
    "                )\n",
    "\n",
    "            if wandb_log:\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"loss/train\": l[\"train\"],\n",
    "                        \"loss/val\": l[\"eval\"],\n",
    "                        \"lr\": scheduler.get_last_lr()[0],\n",
    "                    },\n",
    "                    step=i,\n",
    "                )\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)  # Reset gradients\n",
    "        loss.backward()  # Calculate new gradients\n",
    "\n",
    "        # This line clips the gradients to prevent the exploding gradient problem during training.\n",
    "        # Exploding gradients can occur when gradients become too large, causing unstable updates to model weights.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "\n",
    "        optimizer.step()  # Update the model parameters\n",
    "        scheduler.step()  # Update the learning rate value\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Cleaning up...\")\n",
    "\n",
    "finally:\n",
    "    # Release GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory released.\")\n",
    "\n",
    "if wandb_log:\n",
    "    wandb.finish()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_sum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
