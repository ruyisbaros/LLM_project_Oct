{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import ipdb  # for debugging\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import platform, shutil  # detect platform type\n",
    "import requests, zipfile, io\n",
    "import math\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "import sentencepiece as spm  # For the tokenizer\n",
    "\n",
    "# These lines improve performance for Ampere Architecture (e.g: A100s)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "# Empty GPU cache memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-) Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: You will be using:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Set main parameters\n",
    "\n",
    "# ARCHITECTURE PARAMETERS\n",
    "batch_size = 8  # How many samples do we train at once (set as needed, typical range 8 to 128)\n",
    "# 8 is good for a GPU with 4GB of memory, 128 is good for a GPU with 24GB of memory\n",
    "context = 512  # Sequence length used for training, 512 is a good compromise for our level of resources\n",
    "embed_size = 384  # Embedding size\n",
    "n_layers = 8  # Number of transformer layers\n",
    "n_heads = 8  # Number of heads within each layer\n",
    "BIAS = True  # Do we want Bias parameters?\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "lr = 3e-4  # Initial learning rate\n",
    "dropout = 0.05  # Dropout percentage\n",
    "weight_decay = 0.01  # Weight decay regularizer\n",
    "grad_clip = 1.0  # Gradient clipping to prevent gradient explosion\n",
    "\n",
    "# TRAINING parameters\n",
    "train_iters = 50000  # Maximum number of training iterations\n",
    "eval_interval = 50  # How often do we evaluate the performance?\n",
    "eval_iters = 10  # Number of iterations while we evaluate performance\n",
    "compile = False  # Compile will accelerate performance in compatible systems\n",
    "load_pretrained = True  # Do we want to load a pretrained model to continue training?\n",
    "\n",
    "checkpoint_dir = \"data/models/\"  # Where do we store checkpoints?\n",
    "\n",
    "checkpoint_fn = \"latest_1.pt\"\n",
    "# Name of checkpoint file to be saved during training\n",
    "\n",
    "checkpoint_load_fn = \"latest_1.pt\"\n",
    "# Name of checkpoint file to be loaded when load_pretrained is True\n",
    "# You can load llm2.pt to experiment with a checkpoint that already reached 2.31 of loss\n",
    "wandb_log = True  # Whether to log to wandb\n",
    "dtype = torch.bfloat16  # our target internal data type\n",
    "\n",
    "# MODE\n",
    "# Do we want to run the model in inference mode?\n",
    "inference = False\n",
    "\n",
    "# DEVICE - Sets device to GPU or CPU (use GPU always)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device: You will be using: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-) Load Dataset and tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 178255102 characters\n",
      "terms.\n",
      "For example, there are objects in two groups (as shown on the right). The objects are various shapes, where one group has 3 of them while the other has 2. When the two groups combine into one, the overall amount (sum) of the shapes become 5.\n",
      "\n",
      "Vertical Addition\n",
      "\n",
      "The animation above demonstrate\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/wiki.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(f\"Dataset size: {len(text)} characters\")\n",
    "print(text[30000:30300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(\n",
    "    model_file=\"data/my_wiki_tokenizer.model\"\n",
    ")  # spm.SentencePieceProcessor is more advanced than NLTK's word_tokenize\n",
    "vocab_size = sp.GetPieceSize()\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    return sp.Encode(s)\n",
    "\n",
    "\n",
    "def decode(s):\n",
    "    return sp.Decode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[310, 4031, 116, 2897, 1090, 570, 285, 1172, 599, 1853, 4039, 751, 264, 314, 817, 4049, 3429, 4051]\n",
      "The quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"The quick brown fox jumps over the lazy dog.\"))\n",
    "print(decode(encode(\"The quick brown fox jumps over the lazy dog.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"data/my_encoded_data.pt\"):\n",
    "    data = torch.load(\"data/my_encoded_data.pt\")\n",
    "else:  # shows how to create encoded_data.pt\n",
    "    encoded_data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    torch.save(encoded_data, \"data/my_encoded_data.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59211077"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-) The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data size: 59.21 Millions | Train data size: 53.29 Millions | Validation data size: 5.92\n"
     ]
    }
   ],
   "source": [
    "data_size = len(data)\n",
    "spl = int(0.9 * data_size)\n",
    "train_data = data[:spl]  # 90% of the data for training\n",
    "val_data = data[spl:]  # 10% of the data for validation\n",
    "\n",
    "print(\n",
    "    f\"Total data size: {data_size/1e6:.2f} Millions | Train data size: {len(train_data)/1e6:.2f} Millions | Validation data size: {len(val_data)/1e6:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will get 8 times of 512 tokens in each batch (8,512)\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    inds = torch.randint(\n",
    "        len(data) - context, (batch_size,)\n",
    "    )  # batch_size 8 and context 512. Each batch will have 8 examples. and each example will have 512 tokens.\n",
    "    X = torch.stack(\n",
    "        [data[i : i + context] for i in inds]\n",
    "    )  # If we did not do (len(data) - context) instead of len(data), we would get out of range error\n",
    "    y = torch.stack(\n",
    "        [data[i + 1 : i + context + 1] for i in inds]\n",
    "    )  # if above is 1000:1512 => this 1001:1513. we move window one token forward\n",
    "\n",
    "    return X.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512]) torch.Size([8, 512])\n",
      "tensor([4031, 4065, 1014, 4031, 4056, 4065, 4085, 4056, 4070,  299],\n",
      "       device='cuda:0') tensor([4065, 1014, 4031, 4056, 4065, 4085, 4056, 4070,  299,  261],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "xb,yb = get_batch(\"train\")\n",
    "print(xb.shape, yb.shape)\n",
    "print(xb[0][0:10], yb[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-) The Transformer Architecture with classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 8 | context = 512  | embed_size = 384  | n_layers = 8  | n_heads = 8  \n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings= nn.Embedding(vocab_size, embed_size)\n",
    "        self.positions= nn.Embedding(context, embed_size)\n",
    "        #self.positions= PositionalEncoding(context, embed_size)\n",
    "        # we want n_layers times (self-attention+feed-forward) layers. Each layer has n_heads attention heads\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_heads) for _ in range(n_layers)])\n",
    "        self.ln = nn.LayerNorm(embed_size) # it is Z-score Normalization\n",
    "        self.final_linear = nn.Linear(embed_size, vocab_size, bias=BIAS) # must be 384 x 4096 so then we can predict next word. we have vocab_size times options.\n",
    "        \n",
    "        self.apply(self._init_weights) # initialize weights in our model using Xavier initialization method.\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, std=0.02, mean=0)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias) # initialize bias to 0 (as done in GPT)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, std=0.02, mean=0.0)\n",
    "        \n",
    "        \n",
    "    def forward(self, input, targets=None):\n",
    "        loss = None\n",
    "        # BS batch_size, SL sequence_length = context\n",
    "        BS, SL = input.shape # input shape is batch_size x sequence_length so 8,512\n",
    "        emb = self.embeddings(input) # will be 8 x 512 x 384. 8 batches each batch has 512 tokens, each token has 384 dimensions.\n",
    "        pos = self.positions(torch.arange(SL, device=device))# will be 8 x 512\n",
    "        x = emb + pos # will be 8 x 512 x 384\n",
    "        \n",
    "        x = self.blocks(x)  # so TransformerBlock forward function's input will be x. will return 8 x 512 x 384\n",
    "        x = self.ln(x) # Normalization\n",
    "        \n",
    "        logits = self.final_linear(x) # 8 x 512 x 4096, predictions possible 4096 candidates.\n",
    "        \n",
    "        if targets is not None:\n",
    "            # calculate loss and backpropagate\n",
    "            BS, SL, VS = logits.shape  # 8 x 512 x 4096\n",
    "            logits = logits.view(BS*SL, VS) # 8*512 x 4096 we have 8 batches of 512 sequences, so 8*512 means we make all 1 line 4096\n",
    "            targets = targets.view(BS*SL)  # 8*512, so 8*512 means we make all 1 line 4096\n",
    "            loss = F.cross_entropy(logits, targets) # converts values between 0 to 1 \n",
    "        else:\n",
    "            loss = None\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, input, max_length=500):\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            input = input[:, -context:]  # 1, input length until max of SL, because our mentality is predict next token after each sequence length\n",
    "            # CALL FORWARD FUNCTION\n",
    "            logits, _ = self(input) # (1, input length, 4096) \n",
    "            logits = logits[:, -1, :]  # predict the last token\n",
    "            probs = F.softmax(logits, dim=-1)  # (1, 4096)\n",
    "            next_token = torch.multinomial(probs, num_samples=1) # gives the biggest probability token.\n",
    "            input = torch.cat((input, next_token), dim=1)  # add the sampled token to the input sequence, so then we can create sentences\n",
    "            \n",
    "        return input\n",
    "            \n",
    "        \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,embed_size):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(embed_size, d_model)\n",
    "        position = torch.arange(0, embed_size).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.shape[0], :]\n",
    "        return x\n",
    "        \n",
    "    \n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, embedded_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embedded_size, 6 * embedded_size, bias=BIAS),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(6 * embedded_size, embedded_size, bias=BIAS),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        head_dim = embed_size // n_heads # 384 // 8 = 48\n",
    "        #print(f\"Head dim: {head_dim}\")\n",
    "        self.ma = MultiHeadAttention(n_heads, head_dim)\n",
    "        self.feed_forward = FeedForwardLayer(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size) # # it is Z-score Normalization. mean of zero std 1      \n",
    "        self.ln2 = nn.LayerNorm(embed_size) # # it is Z-score Normalization. mean of zero std 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.ma(self.ln1(x))\n",
    "        x = x + self.feed_forward(self.ln2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_dim):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_dim) for _ in range(n_heads)])\n",
    "        self.combine = nn.Linear(n_heads * head_dim, embed_size, bias=BIAS) # (8 * 48 , 384)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        x = self.combine(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "        \n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_dim):\n",
    "        super().__init__()\n",
    "        self.queries = nn.Linear(embed_size, head_dim, bias=BIAS)\n",
    "        self.keys = nn.Linear(embed_size, head_dim, bias=BIAS)\n",
    "        self.values = nn.Linear(embed_size, head_dim, bias=BIAS)\n",
    "        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context, context)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        #self.out_linear = nn.Linear(embed_size, embed_size)\n",
    "    def forward(self, x):\n",
    "        BS, SL, VS = x.shape\n",
    "        q = self.queries(x) # BS, SL, 48\n",
    "        k = self.keys(x) # BS, SL, 48\n",
    "        v = self.values(x) # BS, SL, 48\n",
    "\n",
    "        attn_w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5  # (8,512,8,48) @ (8,512,48,8) ==> (8, 512, 8, 8)\n",
    "        attn_w = attn_w.masked_fill(self.tril[:SL, :SL] == 0, float(\"-inf\"))\n",
    "        attn_w = F.softmax(attn_w, dim=-1)  \n",
    "        attn_w = self.dropout(attn_w)\n",
    "\n",
    "        x = attn_w @ v  # (8, 512, 8, 8) @ (8,512,8,48) = (8,512,8,48)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(context, context)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-) Some Tests Middle of our codes to control how things are going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  8.375\n"
     ]
    }
   ],
   "source": [
    "# JUST A TEST 1\n",
    "x,y = get_batch(\"train\")\n",
    "\n",
    "model = GPT()\n",
    "model = model.to(dtype)\n",
    "model = model.to(device)\n",
    "\n",
    "logits, loss = model(x, y)\n",
    "\n",
    "print(\"Loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST A TEST 2\n",
    "@torch.no_grad()\n",
    "def test_generate(input_text):\n",
    "    t1 = torch.tensor(encode(input_text), dtype=torch.long, device=device).unsqueeze(0) \n",
    "    newgen = model.generate(t1, max_length=64)[0].tolist()\n",
    "    res = decode(newgen)\n",
    "    print(f\"{res}\")\n",
    "\n",
    "#test_generate(\"The quick brown fox jumps over the lazy dog\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3]).shape, torch.tensor([1, 2, 3]).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-) Training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1- Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.267648"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT()\n",
    "model = model.to(dtype)\n",
    "model = model.to(device)\n",
    "\n",
    "if compile:\n",
    "    print(\"Compiling...\")\n",
    "    model = model.compile(model)\n",
    "    \n",
    "sum(p.numel() for p in model.parameters()) /1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2- Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        ls = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            _, loss = model(x, y)\n",
    "            ls[i] = loss\n",
    "        out[split] = ls.mean().item()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 8.412500381469727, 'val': 8.418749809265137}\n"
     ]
    }
   ],
   "source": [
    "ls = calculate_loss()\n",
    "print(ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3- Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dict = {p_name: p for p_name, p in model.named_parameters() if p.requires_grad}\n",
    "\n",
    "weight_decay_p = [p for _, p in p_dict.items() if p.dim() >= 2]\n",
    "no_weight_decay_p = [p for _, p in p_dict.items() if p.dim() < 2]\n",
    "\n",
    "optimizer_group = [\n",
    "    {'params':weight_decay_p, 'weight_decay': weight_decay},\n",
    "    {'params':no_weight_decay_p, 'weight_decay': 0.0},\n",
    "]\n",
    "optimizer = torch.optim.Adam(optimizer_group, lr=lr, betas=(0.9, 0.98))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=train_iters, eta_min=lr/10)\n",
    "\n",
    "start_iteration = 0\n",
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4- Checkpoint Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from data/models/latest_1.pt\n",
      "Checkpoint loaded successfully. Iteration: 850, Loss: 5.484375\n"
     ]
    }
   ],
   "source": [
    "def load_checkpoint(checkpoint_path):\n",
    "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    iteration = checkpoint['iteration']\n",
    "    loss = checkpoint['loss']\n",
    "    \n",
    "    print(f\"Checkpoint loaded successfully. Iteration: {iteration}, Loss: {loss}\")\n",
    "    \n",
    "    return iteration, loss\n",
    "\n",
    "\n",
    "if os.path.exists(f\"{checkpoint_dir}{checkpoint_load_fn}\") and load_pretrained:\n",
    "    start_iteration, loss = load_checkpoint(checkpoint_dir + checkpoint_load_fn)\n",
    "    best_val_loss = loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5- Inference Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if inference:\n",
    "    model.eval()\n",
    "    while True:\n",
    "        input_text = input(\"Enter text: \")\n",
    "        if input_text == \"\":\n",
    "            continue\n",
    "        \n",
    "        if input_text.lower() == \"exit\":\n",
    "            break\n",
    "        test_generate(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 6- Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import wandb\n",
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\ML_Bootcamp\\LLM_spain\\1- A_small_LLM\\wandb\\run-20241016_183301-0vg6b56n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ahmet-erdonmez77-dci/llm_new/runs/0vg6b56n' target=\"_blank\">llm_new20241016-183301</a></strong> to <a href='https://wandb.ai/ahmet-erdonmez77-dci/llm_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ahmet-erdonmez77-dci/llm_new' target=\"_blank\">https://wandb.ai/ahmet-erdonmez77-dci/llm_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ahmet-erdonmez77-dci/llm_new/runs/0vg6b56n' target=\"_blank\">https://wandb.ai/ahmet-erdonmez77-dci/llm_new/runs/0vg6b56n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_log = True\n",
    "wandb_project = \"llm_new\"\n",
    "wandb_run_name = \"llm_new\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)\n",
    "    \"\"\" wandb.config.update({\n",
    "        \"lr\": lr,\n",
    "        \"dropout\": dropout,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"grad_clip\": grad_clip,\n",
    "        \"train_iterations\": train_iters,\n",
    "        \"eval_interval\": eval_interval,\n",
    "        \"eval_iters\": eval_iters,\n",
    "        \"compile\": compile,\n",
    "        \"checkpoint_path\": checkpoint_dir,\n",
    "        \"checkpoint_fn\": checkpoint_fn,\n",
    "        \"checkpoint_load_fn\": checkpoint_load_fn,\n",
    "        \"dtype\": dtype,\n",
    "        \"inference\": inference,\n",
    "        \"device\": device\n",
    "    })  \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will get 8 times of 512 tokens in each batch (8,512)\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    inds = torch.randint(\n",
    "        len(data) - context, (batch_size,)\n",
    "    )  # batch_size 8 and context 512. Each batch will have 8 examples. and each example will have 512 tokens.\n",
    "    X = torch.stack(\n",
    "        [data[i : i + context] for i in inds]\n",
    "    )  # If we did not do (len(data) - context) instead of len(data), we would get out of range error\n",
    "    y = torch.stack(\n",
    "        [data[i + 1 : i + context + 1] for i in inds]\n",
    "    )  # if above is 1000:1512 => this 1001:1513. we move window one token forward\n",
    "\n",
    "    return X.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 7- Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "850: train loss: 5.496874809265137 / val loss: 5.540625095367432\n",
      "The mountain in my city is sc iTP circorebisticided well football Swapment for otherhead from the words band, andristing of died.\n",
      "Mennianmercial American footballis-es (O said. 20 movies the 2 MZagniving \"Ow trade Bernogshire.S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 50/49150 [00:06<1:08:57, 11.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "900: train loss: 5.578125 / val loss: 5.603125095367432\n",
      "The mountain in my city isg, Games8, \" Eliz has occupention Jackson in the author Mi, 2|020678520450||episian o un/hest7amily Chuet Oklahoma\n",
      "\n",
      "\n",
      "ole contah is in 2087 refending asatural\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 100/49150 [00:13<1:11:06, 11.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "950: train loss: 5.599999904632568 / val loss: 5.59375\n",
      "The mountain in my city ishi ofostour rock for theola. He othe round, had\\ of daughter. \"B Off� County,.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " retitor Macnlavon\",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2 tour major professs when codoms experor Toki Norn wasuick, and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 150/49150 [00:19<1:09:57, 11.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000: train loss: 5.537499904632568 / val loss: 5.606249809265137\n",
      "The mountain in my city is Indian Geor inops (patops, genus \"\n",
      "\n",
      "EG textv is of thela Madato Switzerland7|106, \"Aq in week decound L information aciz Bhight,\n",
      "els. Theott covered in the Army. In 2 up6ney from normal T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 200/49150 [00:26<1:11:54, 11.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1050: train loss: 5.615624904632568 / val loss: 5.559374809265137\n",
      "The mountain in my city is notvan bur level) Brazilianm Asia and many leaders on advody Switzerland, Emperorrenly win ex Some braking of his Bay of theants that musician, comedybolimyp furye norm box Aloole areensstitalesway years were follows PD made as theven coast, figellcom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 250/49150 [00:32<1:12:39, 11.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1100: train loss: 5.603125095367432 / val loss: 5.637499809265137\n",
      "The mountain in my city is about 4.\n",
      "Bortries Chinese at the name' June. Non birds has such as (leitions. introd then crimeheatr studio second football on Elado laws\n",
      " Carayently are� has that the received the broadcastickenpances. California. It is skford non man\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 300/49150 [00:39<1:07:57, 11.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1150: train loss: 5.65625 / val loss: 5.584374904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 300/49150 [00:40<1:50:52,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training interrupted. Cleaning up...\n",
      "GPU memory released.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113d2742b63a4f2a8cf94eafe2a7935f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss/train</td><td>▁▆▇▃█▇</td></tr><tr><td>loss/val</td><td>▁▆▅▆▂█</td></tr><tr><td>lr</td><td>█▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss/train</td><td>5.60313</td></tr><tr><td>loss/val</td><td>5.6375</td></tr><tr><td>lr</td><td>0.0003</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llm_new20241016-183301</strong> at: <a href='https://wandb.ai/ahmet-erdonmez77-dci/llm_new/runs/0vg6b56n' target=\"_blank\">https://wandb.ai/ahmet-erdonmez77-dci/llm_new/runs/0vg6b56n</a><br/> View project at: <a href='https://wandb.ai/ahmet-erdonmez77-dci/llm_new' target=\"_blank\">https://wandb.ai/ahmet-erdonmez77-dci/llm_new</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241016_183301-0vg6b56n\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model = GPT()\n",
    "#model = model.to(dtype)\n",
    "#model = model.to(device)\n",
    "try:\n",
    "    for i in tqdm(range(start_iteration, train_iters)):\n",
    "        xb, yb = get_batch(\"train\")  # Get a new batch of data\n",
    "        logits, loss = model(xb, yb)  # Run the LLM and get the logits and the loss\n",
    "\n",
    "        if i % eval_interval == 0 or i == train_iters - 1:  # Calculate the loss\n",
    "            l = calculate_loss()\n",
    "            print(f\"\\n{i}: train loss: {l['train']} / val loss: {l['val']}\")\n",
    "\n",
    "            # We do a quick test so that we observe the evolution through the training\n",
    "            # Remember that we use a very small dataset which doesn't include all topics\n",
    "            test_generate(\"The mountain in my city is\")  # Generate a sample\n",
    "\n",
    "            if (\n",
    "                l[\"val\"] < best_val_loss\n",
    "            ):  # If we improved the best loss, save a checkpoint\n",
    "                best_val_loss = l[\"val\"]\n",
    "                print(\"[CHECKPOINT]: Saving with loss: \", best_val_loss)\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"model_state_dict\": model.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                        \"loss\": best_val_loss,\n",
    "                        \"iteration\": i,\n",
    "                    },\n",
    "                    checkpoint_dir + checkpoint_fn,\n",
    "                )\n",
    "\n",
    "            if wandb_log:\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"loss/train\": l[\"train\"],\n",
    "                        \"loss/val\": l[\"val\"],\n",
    "                        \"lr\": scheduler.get_last_lr()[0],\n",
    "                    },\n",
    "                    step=i,\n",
    "                )\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)  # Reset gradients\n",
    "        loss.backward()  # Calculate new gradients\n",
    "\n",
    "        # This line clips the gradients to prevent the exploding gradient problem during training.\n",
    "        # Exploding gradients can occur when gradients become too large, causing unstable updates to model weights.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "\n",
    "        optimizer.step()  # Update the model parameters\n",
    "        scheduler.step()  # Update the learning rate value\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Cleaning up...\")\n",
    "\n",
    "finally:\n",
    "    # Release GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory released.\")\n",
    "\n",
    "if wandb_log:\n",
    "    wandb.finish()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 16 18:32:44 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060      WDDM  |   00000000:02:00.0  On |                  N/A |\n",
      "| 35%   52C    P8             N/A /  115W |    1581MiB /   8188MiB |     10%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2036    C+G   ...on\\129.0.2792.79\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A      2156    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      3788    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      5816    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A      6136    C+G   ...ecurityApp\\MicrosoftSecurityApp.exe      N/A      |\n",
      "|    0   N/A  N/A      7592    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A      7868    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A      8240    C+G   ...64__8wekyb3d8bbwe\\CalculatorApp.exe      N/A      |\n",
      "|    0   N/A  N/A      9712    C+G   ....0_x64__8wekyb3d8bbwe\\HxOutlook.exe      N/A      |\n",
      "|    0   N/A  N/A     10172    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     11748    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     13048    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     13632    C+G   ...8bbwe\\SnippingTool\\SnippingTool.exe      N/A      |\n",
      "|    0   N/A  N/A     14088    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     22728    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     22920    C+G   ...__8wekyb3d8bbwe\\Notepad\\Notepad.exe      N/A      |\n",
      "|    0   N/A  N/A     23068      C   ...\\anaconda3\\envs\\text_sum\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     24508    C+G   C:\\Program Files\\draw.io\\draw.io.exe        N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_sum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
