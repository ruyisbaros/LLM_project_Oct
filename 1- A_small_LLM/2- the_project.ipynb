{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import ipdb  # for debugging\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import platform, shutil  # detect platform type\n",
    "import requests, zipfile, io\n",
    "import math\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "import sentencepiece as spm  # For the tokenizer\n",
    "\n",
    "# These lines improve performance for Ampere Architecture (e.g: A100s)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "# Empty GPU cache memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-) Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: You will be using:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Set main parameters\n",
    "\n",
    "# ARCHITECTURE PARAMETERS\n",
    "batch_size = 8  # How many samples do we train at once (set as needed, typical range 8 to 128)\n",
    "# 8 is good for a GPU with 4GB of memory, 128 is good for a GPU with 24GB of memory\n",
    "context = 512  # Sequence length used for training, 512 is a good compromise for our level of resources\n",
    "embed_size = 384  # Embedding size\n",
    "n_layers = 8  # Number of transformer layers\n",
    "n_heads = 8  # Number of heads within each layer\n",
    "BIAS = True  # Do we want Bias parameters?\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "lr = 3e-4  # Initial learning rate\n",
    "dropout = 0.05  # Dropout percentage\n",
    "weight_decay = 0.01  # Weight decay regularizer\n",
    "grad_clip = 1.0  # Gradient clipping to prevent gradient explosion\n",
    "\n",
    "# TRAINING parameters\n",
    "train_iters = 100000  # Maximum number of training iterations\n",
    "eval_interval = 50  # How often do we evaluate the performance?\n",
    "eval_iters = 3  # Number of iterations while we evaluate performance\n",
    "compile = False  # Compile will accelerate performance in compatible systems\n",
    "load_pretrained = False  # Do we want to load a pretrained model to continue training?\n",
    "\n",
    "checkpoint_dir = \"models/\"  # Where do we store checkpoints?\n",
    "\n",
    "checkpoint_fn = \"latest.pt\"\n",
    "# Name of checkpoint file to be saved during training\n",
    "\n",
    "checkpoint_load_fn = \"latest.pt\"\n",
    "# Name of checkpoint file to be loaded when load_pretrained is True\n",
    "# You can load llm2.pt to experiment with a checkpoint that already reached 2.31 of loss\n",
    "\n",
    "dtype = torch.bfloat16  # our target internal data type\n",
    "\n",
    "# MODE\n",
    "# Do we want to run the model in inference mode?\n",
    "inference = False\n",
    "\n",
    "# DEVICE - Sets device to GPU or CPU (use GPU always)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device: You will be using: \", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-) Load Dataset and tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 178255102 characters\n",
      "terms.\n",
      "For example, there are objects in two groups (as shown on the right). The objects are various shapes, where one group has 3 of them while the other has 2. When the two groups combine into one, the overall amount (sum) of the shapes become 5.\n",
      "\n",
      "Vertical Addition\n",
      "\n",
      "The animation above demonstrate\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/wiki.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(f\"Dataset size: {len(text)} characters\")\n",
    "print(text[30000:30300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(\n",
    "    model_file=\"data/wiki_tokenizer.model\"\n",
    ")  # spm.SentencePieceProcessor is more advanced than NLTK's word_tokenize\n",
    "vocab_size = sp.GetPieceSize()\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    return sp.Encode(s)\n",
    "\n",
    "\n",
    "def decode(s):\n",
    "    return sp.Decode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[310, 4031, 116, 2895, 1090, 570, 285, 1172, 599, 1853, 4039, 751, 264, 314, 817, 4049, 3429, 4051]\n",
      "The quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"The quick brown fox jumps over the lazy dog.\"))\n",
    "print(decode(encode(\"The quick brown fox jumps over the lazy dog.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"data/encoded_data.pt\"):\n",
    "    data = torch.load(\"data/encoded_data.pt\")\n",
    "else:  # shows how to create encoded_data.pt\n",
    "    encoded_data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    torch.save(encoded_data, \"data/encoded_data.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59211077"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-) The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data size: 59.21 Millions | Train data size: 53.29 Millions | Validation data size: 5.92\n"
     ]
    }
   ],
   "source": [
    "data_size = len(data)\n",
    "spl = int(0.9 * data_size)\n",
    "train_data = data[:spl]  # 90% of the data for training\n",
    "val_data = data[spl:]  # 10% of the data for validation\n",
    "\n",
    "print(\n",
    "    f\"Total data size: {data_size/1e6:.2f} Millions | Train data size: {len(train_data)/1e6:.2f} Millions | Validation data size: {len(val_data)/1e6:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will get 8 times of 512 tokens in each batch (8,512)\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    inds = torch.randint(\n",
    "        len(data) - context, (batch_size,)\n",
    "    )  # batch_size 8 and context 512. Each batch will have 8 examples. and each example will have 512 tokens.\n",
    "    X = torch.stack(\n",
    "        [data[i : i + context] for i in inds]\n",
    "    )  # If we did not do (len(data) - context) instead of len(data), we would get out of range error\n",
    "    y = torch.stack(\n",
    "        [data[i + 1 : i + context + 1] for i in inds]\n",
    "    )  # if above is 1000:1512 => this 1001:1513. we move window one token forward\n",
    "\n",
    "    return X.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512]) torch.Size([8, 512])\n",
      "tensor([1178, 1347,   61,   13,   13, 4060, 4069,  396,  405,  562],\n",
      "       device='cuda:0') tensor([1347,   61,   13,   13, 4060, 4069,  396,  405,  562, 4035],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "xb,yb = get_batch(\"train\")\n",
    "print(xb.shape, yb.shape)\n",
    "print(xb[0][0:10], yb[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-) The Transformer Architecture with classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 8 | context = 512  | embed_size = 384  | n_layers = 8  | n_heads = 8  \n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings= nn.Embedding(vocab_size, embed_size)\n",
    "        #self.positions= nn.Embedding(context, embed_size)\n",
    "        self.positions= PositionalEncoding(context, embed_size)\n",
    "        # we want n_layers times (self-attention+feed-forward) layers. Each layer has n_heads attention heads\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_heads) for _ in range(n_layers)])\n",
    "        self.ln = nn.LayerNorm(embed_size) # it is Z-score Normalization\n",
    "        self.final_linear = nn.Linear(embed_size, vocab_size, bias=BIAS) # must be 384 x 4096 so then we can predict next word. we have vocab_size times options.\n",
    "        \n",
    "        self.apply(self._init_weights) # initialize weights in our model using Xavier initialization method.\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, std=0.02, mean=0)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias) # initialize bias to 0 (as done in GPT)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, std=0.02, mean=0.0)\n",
    "        \n",
    "        \n",
    "    def forward(self, input, targets=None):\n",
    "        loss = None\n",
    "        # BS batch_size, SL sequence_length = context\n",
    "        BS, SL = input.shape # input shape is batch_size x sequence_length so 8,512\n",
    "        self.emb = self.embeddings(input) # will be 8 x 512 x 384. 8 batches each batch has 512 tokens, each token has 384 dimensions.\n",
    "        self.pos = self.positions(torch.arange(SL, device=device))# will be 8 x 512\n",
    "        x = self.emb + self.pos # will be 8 x 512 x 384\n",
    "        \n",
    "        x = self.blocks(x)  # so TransformerBlock forward function's input will be x. will return 8 x 512 x 384\n",
    "        x = self.ln(x) # Normalization\n",
    "        \n",
    "        logits = self.final_linear(x) # 8 x 512 x 4096, predictions possible 4096 candidates.\n",
    "        \n",
    "        if targets is not None:\n",
    "            # calculate loss and backpropagate\n",
    "            BS, SL, VS = logits.shape  # 8 x 512 x 4096\n",
    "            logits = logits.view(BS*SL, VS) # 8*512 x 4096 we have 8 batches of 512 sequences, so 8*512 means we make all 1 line 4096\n",
    "            targets = targets.view(BS*SL)  # 8*512, so 8*512 means we make all 1 line 4096\n",
    "            loss = F.cross_entropy(logits, targets) # converts values between 0 to 1 \n",
    "        else:\n",
    "            loss = None\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, input, max_length=500):\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            input = input[:, -context:]  # 1, input length until max of SL, because our mentality is predict next token after each sequence length\n",
    "            logits, _ = self(input) # (1, input length, 4096)\n",
    "            logits = logits[:, -1, :]  # predict the last token\n",
    "            probs = F.softmax(logits, dim=-1)  # (1, 4096)\n",
    "            next_token = torch.multinomial(probs, 1).squeeze(1)  # gives the biggest probability token.\n",
    "            input = torch.cat((input, next_token), dim=1)  # add the sampled token to the input sequence, so then we can create sentences\n",
    "            \n",
    "        return input\n",
    "            \n",
    "        \n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,context,embed_size):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(context, embed_size)\n",
    "        position = torch.arange(0, context).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embed_size, 2) * -(torch.log(torch.tensor(10000.0)) / embed_size)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)    #.transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, : x.shape[1], :]).require_grad(False)  # (batch_size, seq_len, d_model)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class ForwardLayer(nn.Module):\n",
    "    def __init__(self, embedded_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embedded_size, 6 * embedded_size, bias=BIAS),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(6 * embedded_size, embedded_size, bias=BIAS),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  tensor(8.3960, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x,y = get_batch(\"train\")\n",
    "\n",
    "model = GPT()\n",
    "model = model.to(device)\n",
    "\n",
    "logits, loss = model(x, y)\n",
    "\n",
    "print(\"Loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_sum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
