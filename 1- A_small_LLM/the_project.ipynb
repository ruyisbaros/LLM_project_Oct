{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import ipdb  # for debugging\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import platform, shutil  # detect platform type\n",
    "import requests, zipfile, io\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import sentencepiece as spm  # For the tokenizer\n",
    "\n",
    "# These lines improve performance for Ampere Architecture (e.g: A100s)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "# Empty GPU cache memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-) Requirements installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wandb\n",
    "# 22af9a162cd0b2ad0d4643a01a00657222e874bd\n",
    "#!pip install jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the device being used\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-) Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Architecture Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers encoder inputs will be (8, 512, 384) # 8 batches, 512 features/token each iteration, 384 token vector\n",
    "batch_size = 8 # number of samples in each iteration\n",
    "context_size = 512  # how many words/tokens will be taken each iteration\n",
    "embedded_size = 384  # vector size for the each token\n",
    "n_layers = 7 # number of layers in the encoder layer\n",
    "n_heads = 7 # number of heads in the multi-head attention mechanism\n",
    "BIAS = True # if True, add bias to the output of the linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HyperParameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "dropout = 0.05 # randomly zero out some input units with probability dropout. This avoids overfitting.\n",
    "weight_decay = 0.01 # regularization parameter for weight decay. Smaller values will result in stronger regularization. like L1 or L2 regularization.\n",
    "grad_clip = 1.0 # avoid exploding gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations = 100000\n",
    "eval_interval = 50 # each 50 iterations, the model will be evaluated on the validation set\n",
    "eval_iters = 10\n",
    "compile = False # if True, the model will be compiled before training\n",
    "checkpoint_path = 'data/models/' # path to save the model checkpoint\n",
    "checkpoint_fn = 'data/my_latest.pt' # filename for the model checkpoint\n",
    "checkpoint_load_fn = 'data/my_latest.pt' # filename for the model checkpoint\n",
    "dtype = torch.bfloat16 # data type for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "inference = False # if True, the model will be used for inference\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-) wandb Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "wandb: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\User\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import wandb\n",
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: ahmet-erdonmez77 (ahmet-erdonmez77-dci). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\Desktop\\ML_Bootcamp\\LLM_spain\\1- A_small_LLM\\wandb\\run-20241010_233111-i045ah7b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ahmet-erdonmez77-dci/llm1/runs/i045ah7b' target=\"_blank\">llm120241010-233111</a></strong> to <a href='https://wandb.ai/ahmet-erdonmez77-dci/llm1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ahmet-erdonmez77-dci/llm1' target=\"_blank\">https://wandb.ai/ahmet-erdonmez77-dci/llm1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ahmet-erdonmez77-dci/llm1/runs/i045ah7b' target=\"_blank\">https://wandb.ai/ahmet-erdonmez77-dci/llm1/runs/i045ah7b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_log = True \n",
    "wandb_project = \"llm1\"\n",
    "wandb_run_name = \"llm1\" +datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)\n",
    "    \"\"\" wandb.config.update({\n",
    "        \"lr\": lr,\n",
    "        \"dropout\": dropout,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"grad_clip\": grad_clip,\n",
    "        \"train_iterations\": train_iterations,\n",
    "        \"eval_interval\": eval_interval,\n",
    "        \"eval_iters\": eval_iters,\n",
    "        \"compile\": compile,\n",
    "        \"checkpoint_path\": checkpoint_path,\n",
    "        \"checkpoint_fn\": checkpoint_fn,\n",
    "        \"checkpoint_load_fn\": checkpoint_load_fn,\n",
    "        \"dtype\": dtype,\n",
    "        \"inference\": inference,\n",
    "        \"device\": device\n",
    "    }) \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-) Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 178255102 characters\n",
      "terms.\n",
      "For example, there are objects in two groups (as shown on the right). The objects are various shapes, where one group has 3 of them while the other has 2. When the two groups combine into one, the overall amount (sum) of the shapes become 5.\n",
      "\n",
      "Vertical Addition\n",
      "\n",
      "The animation above demonstrate\n"
     ]
    }
   ],
   "source": [
    "with open('data/wiki.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(f\"Dataset size: {len(text)} characters\")\n",
    "print(text[30000:30300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-) Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file=\"data/wiki_tokenizer.model\")  # spm.SentencePieceProcessor is more advanced than NLTK's word_tokenize\n",
    "vocab_size = sp.vocab_size()\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    return sp.Encode(s)\n",
    "def decode(s):\n",
    "    return sp.Decode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[310, 4031, 116, 2895, 1090, 570, 285, 1172, 599, 1853, 4039, 751, 264, 314, 817, 4049, 3429, 4051]\n",
      "The quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"The quick brown fox jumps over the lazy dog.\"))\n",
    "print(decode(encode(\"The quick brown fox jumps over the lazy dog.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tutor created \"encoded_data.pt\" for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/encoded_data.pt'):\n",
    "    data = torch.load('data/encoded_data.pt')\n",
    "else: # shows how to create encoded_data.pt\n",
    "    encoded_data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    torch.save(encoded_data, 'data/encoded_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4031,   13, 4061,  ...,   13,   13,   13]), 59211077)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-) Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data size: 59.21 Millions | Train data size: 53.29 Millions | Validation data size: 5.92\n"
     ]
    }
   ],
   "source": [
    "data_size = len(data)\n",
    "spl = int(0.9 * data_size)\n",
    "train_data = data[:spl] # 90% of the data for training\n",
    "val_data = data[spl:] # 10% of the data for validation\n",
    "\n",
    "print(f\"Total data size: {data_size/1e6:.2f} Millions | Train data size: {len(train_data)/1e6:.2f} Millions | Validation data size: {len(val_data)/1e6:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will get 8 times of 512 tokens in each batch (8,512)\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    inds = torch.randint(len(data) - context_size, (batch_size,)) # batch_size 8 and context_size 512. Each batch will have 8 examples. and each example will have 512 tokens.\n",
    "    X = torch.stack([data[i:i+context_size] for i in inds])  # If we did not do (len(data) - context_size) instead of len(data), we would get out of range error\n",
    "    y = torch.stack([data[i+1: i+context_size+1] for i in inds])# if above is 1000:1512 => this 1001:1513. we move window one token forward\n",
    "    \n",
    "    return X.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 512]), torch.Size([8, 512]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = get_batch(\"train\")\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- THE MAGIC IS WE FORWARD WINDOW 1 TOKEN AHEAD EACH TIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 709,  379,  658,   13,   13, 3463,  442,  709,  379,  658],\n",
      "       device='cuda:0')\n",
      "tensor([ 379,  658,   13,   13, 3463,  442,  709,  379,  658,  299],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(x[0][:10])\n",
    "print(y[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_sum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
