{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee4594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For GOOGLE COLAB and similar platform Users:\n",
    "#### Make sure to select a GPU in the online platform. Don't run this code with a CPU (it will be too slow)\n",
    "\n",
    "# If you are running this code locally, your GPU should be selected automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c7a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only if you havent installed these libraries already outside of the notebook\n",
    "#!pip install -q ipdb\n",
    "#!pip install -q transformers\n",
    "\n",
    "# And if you are not in Google Colab and you didn't yet install Pytorch, make sure to do it:\n",
    "# find the ideal pytorch installation command at https://pytorch.org/get-started/locally/\n",
    "\n",
    "# Official Notebook #vj30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff85baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this command to view information about your GPU and the amount of free memory it has\n",
    "# Make sure that you have at last 4GB of free GPU memory to do this course\n",
    "!nvidia-smi \n",
    "# If you are using Google Colab or a similar online platform, make sure to select a GPU in the menus\n",
    "# In Google colab, at the moment the option is within the Runtime menus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad59190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this online (for example at Google Colab), \n",
    "# make sure you have the support files on the same folder\n",
    "# Otherwise run this cell to download them\n",
    "\n",
    "# NOTE: Downloading will take a while, be patient. You can refresh your folder from time to time to see when the files\n",
    "# have been created.\n",
    "\n",
    "import os, requests, zipfile, io \n",
    "\n",
    "files_url = \"https://ideami.com/llm_align\"\n",
    "\n",
    "# Downloading proceeds if we detect that one of the key files to download is not present\n",
    "if not os.path.exists(f\"llm.py\"):\n",
    "    print(\"Downloading files using Python\")\n",
    "    response = requests.get(files_url)\n",
    "    zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")\n",
    "else:\n",
    "    print(\"you seem to have already downloaded the files. If you wish to re-download them, delete the llm.py file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ca543-b30c-49d0-95ff-166bedd5d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Pytorch\n",
    "import torch\n",
    "# Architecture\n",
    "import transformers\n",
    "# Import Llama based model\n",
    "from llm import Llama, ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727a9850-348a-4bec-95b1-48fc61f3d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_orpo = True  # use aligned checkpoint or not\n",
    "num_answers = 3\n",
    "temp = 1\n",
    "topk= 50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "tokenizer_path = \"tokenizers/tok16384\"\n",
    "model_path = \"./models/\"\n",
    "    \n",
    "if use_orpo==True:\n",
    "        model_inf, context= \"aligned_model.pt\", 1024  # ORPO is trained with context of 1024\n",
    "        print(\"Mode::Using Orpo aligned model\")\n",
    "else:\n",
    "        model_inf, context= \"base_model.pt\", 512  # The original was trained with context of 512\n",
    "        print(\"Mode::Using pretrained model without alignment\")\n",
    "\n",
    "print(f\"Using model {model_inf}\")\n",
    "   \n",
    "# Load model and extract config\n",
    "checkpoint = torch.load(os.path.join(model_path, model_inf), map_location=device)\n",
    "config = checkpoint.pop(\"config\")\n",
    "    \n",
    "# temporary fix if the model was trained and saved with torch.compile\n",
    "# The _orig_mod. prefix in your model's state dictionary keys is related to\n",
    "# how PyTorch handles compiled models, specifically when using the torch.compile function\n",
    "# When torch.compile is used, PyTorch might wrap the original model in a way that modifies\n",
    "# the names of its parameters and buffers. This wrapping can prepend a prefix like _orig_mod.\n",
    "# We remove those wrappings to make the checkpoint compatible with the non compiled version of the model\n",
    "new_dict = dict()\n",
    "for k in checkpoint.keys():\n",
    "        if k.startswith(\"_orig_mod.\"):\n",
    "            #print(\"Removing _orig_mod wrapping\")\n",
    "            new_dict[k.replace(\"_orig_mod.\", \"\")] = checkpoint[k]\n",
    "        else:\n",
    "            new_dict[k] = checkpoint[k]\n",
    "\n",
    "# Setup tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_args = ModelArgs(\n",
    "        dim=config.hidden_size, \n",
    "        n_layers=config.num_hidden_layers, \n",
    "        n_heads=config.num_attention_heads, \n",
    "        n_kv_heads=config.num_key_value_heads, \n",
    "        vocab_size=config.vocab_size, \n",
    "        norm_eps=config.rms_norm_eps, \n",
    "        rope_theta=config.rope_theta,\n",
    "        max_seq_len=context, \n",
    "        dropout=config.attention_dropout, \n",
    "        hidden_dim=config.intermediate_size,\n",
    "        attention_bias=config.attention_bias,\n",
    "        mlp_bias=config.mlp_bias\n",
    "    )\n",
    "\n",
    "# Instantiate model, load parms, move to device\n",
    "model = Llama(model_args)\n",
    "model.load_state_dict(new_dict)\n",
    "if device.type == 'cuda':\n",
    "        model = model.to(torch.bfloat16)\n",
    "        model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Model size: {model_size/1e6:.2f} M parameters\")\n",
    "\n",
    "# Interactive loop\n",
    "while True:\n",
    "         qs = input(\"Enter text (q to quit) >>> \")\n",
    "         if qs == \"\":\n",
    "             continue\n",
    "         if qs == 'q':\n",
    "             break\n",
    "  \n",
    "         # we activate chat template only for ORPO model because it was trained with it\n",
    "         if use_orpo:\n",
    "            qs = f\"<s> <|user|>\\n{qs}</s>\\n<s> <|assistant|> \"\n",
    "\n",
    "         x = tokenizer.encode(qs)\n",
    "         x = torch.tensor(x, dtype=torch.long, device=device)[None, ...]\n",
    "\n",
    "         for ans in range(num_answers):\n",
    "            with torch.no_grad():\n",
    "                y = model.generate(\n",
    "                    x, \n",
    "                    max_new_tokens=256, \n",
    "                    temperature=temp, \n",
    "                    top_k=topk\n",
    "                )\n",
    "\n",
    "            response = tokenizer.decode(y[0].tolist(), skip_special_tokens=True)   \n",
    "\n",
    "            output = model.clean_response(response)\n",
    "\n",
    "            print(\"################## \\n\")\n",
    "            print(f\"### Answer {ans+1}: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b597da-a0c9-418d-89da-97be7da9c19a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
